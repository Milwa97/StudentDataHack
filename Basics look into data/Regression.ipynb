{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.linear_model import LinearRegression as Lin_Reg\n",
    "from sklearn.linear_model import Ridge as Ridge_Reg\n",
    "from sklearn.linear_model import Lasso as Lasso_Reg\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "# To plot pretty figures\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cmx\n",
    "import matplotlib.colors as colors\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "plt.rcParams['figure.figsize']=12,10\n",
    "\n",
    "N = 256\n",
    "vals = np.ones((N, 4))\n",
    "vals[:, 0] = np.linspace(255/256, 1, N)    ## red  255,192, 203\n",
    "vals[:, 1] = np.linspace(192/256, 1, N)\n",
    "vals[:, 2] = np.linspace(203/256, 1, N)  ## blue 75, 0, 130\n",
    "\n",
    "Pinks = ListedColormap(vals)\n",
    "top = cm.get_cmap(Pinks, 256)\n",
    "bottom = cm.get_cmap('Purples', 256)\n",
    "newcolors = np.vstack((top(np.linspace(0, 1, 256)),\n",
    "                       bottom(np.linspace(0, 1, 256))))\n",
    "newcmp = ListedColormap(newcolors, name='PinkPueple')\n",
    "\n",
    "np.random.seed(17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data_normalized.csv')\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = np.corrcoef(data.T)\n",
    "pd.DataFrame(corr_matrix)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(corr_matrix, cmap = newcmp, vmax = 1, vmin = -1)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training set, validation set, testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data_x: np.ndarray, data_y: np.ndarray):\n",
    "    \n",
    "    ordering = np.arange(data_x.shape[0])\n",
    "    np.random.shuffle(ordering)\n",
    "    data_x = data_x[ordering]\n",
    "    data_y = data_y[ordering]\n",
    "    \n",
    "    valid_start = int(len(data_x) * 0.7)\n",
    "    test_start = int(len(data_x) * 0.9)\n",
    "    \n",
    "    train_set = (data_x[:valid_start], data_y[:valid_start])\n",
    "    valid_set = (data_x[valid_start:test_start], data_y[valid_start:test_start])\n",
    "    test_set = (data_x[test_start:], data_y[test_start:])\n",
    "    \n",
    "    return train_set, valid_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y = np.array(data['target'])\n",
    "x = np.array(data.drop(['target'], axis=1))\n",
    "\n",
    "train_set, valid_set, test_set = split_data(x,y)\n",
    "\n",
    "print(train_set[0].shape)\n",
    "print(valid_set[0].shape)\n",
    "print(test_set[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_set[0]\n",
    "y_train = train_set[1]\n",
    "\n",
    "x_val = valid_set[0]\n",
    "y_val = valid_set[1]\n",
    "\n",
    "x_test = test_set[0]\n",
    "y_test = test_set[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Linear Regression - worth try :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = Lin_Reg()\n",
    "reg.fit(x_train, y_train)\n",
    "coefficients = reg.coef_\n",
    "reg.predict(x_val)\n",
    "\n",
    "print('Simple Linear Regression')\n",
    "#print('\\nCoefficients: {:}'.format(coefficients))\n",
    "\n",
    "output = np.round(reg.predict(x_train) ).astype(int)\n",
    "target = y_train\n",
    "accuracy = sum(output == target)/ len(target)\n",
    "print(\"\\n\\nAccuracy on train set: {:.2f} %\".format(accuracy*100))\n",
    "\n",
    "output = np.round(reg.predict(x_val) ).astype(int)\n",
    "target = y_val\n",
    "accuracy = sum(output == target)/ len(target)\n",
    "print(\"\\n\\nAccuracy on validation set: {:.2f} %\".format(accuracy*100))\n",
    "\n",
    "print('\\nAverage Cross Validation in training set:\\t{:}'.format(np.average(cross_val_score(reg,x_train, y_train, scoring ='r2',cv = 5))))\n",
    "print('Average Cross Validation in test set:\\t{:}'.format(np.average(cross_val_score(reg,x_test, y_test, scoring ='r2',cv = 5))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for different values of lambda \n",
    "lambda_min = -5\n",
    "lambda_max = 10\n",
    "eta = 10\n",
    "\n",
    "num_lambdas = eta * (lambda_max- lambda_min)\n",
    "num_predictors = x.shape[1]\n",
    "lambdas= np.linspace(lambda_min,lambda_max, num_lambdas)\n",
    "\n",
    "train_accuracy = np.zeros(num_lambdas)\n",
    "val_accuracy = np.zeros(num_lambdas)\n",
    "coeff_a = np.zeros((num_lambdas, num_predictors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind, i in enumerate(lambdas):    \n",
    "    # Fit ridge regression on train set\n",
    "    reg = Ridge_Reg(alpha = 10**i)\n",
    "    reg.fit(x_train, y_train)\n",
    "       \n",
    "    coeff_a[ind,:] = reg.coef_\n",
    "    \n",
    "    # Evaluate train & test performance\n",
    "    output = np.round(reg.predict(x_train)).astype(int)\n",
    "    target = y_train\n",
    "    train_accuracy[ind] = sum(output == target)/ len(target)\n",
    "    \n",
    "    output = np.round(reg.predict(x_val)).astype(int)\n",
    "    target = y_val\n",
    "    val_accuracy[ind] = sum(output == target)/ len(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "plt.figure(figsize=(18, 8))\n",
    "\n",
    "plt.plot(lambdas, train_accuracy, 'bo-', label=r'accuracy training set', color=\"violet\", alpha=0.6, linewidth=3)\n",
    "plt.plot(lambdas, val_accuracy, 'bo-', label=r'accuracy val set', color=\"darkviolet\", alpha=0.6, linewidth=3)\n",
    "\n",
    "plt.xlabel('Lambda value'); plt.ylabel(r'accuracy')\n",
    "plt.xlim(lambda_min, lambda_max)\n",
    "plt.title(r'Evaluate ridge regression with different lamdas')\n",
    "plt.legend(loc='best')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=30\n",
    "\n",
    "C_arr = np.linspace(-10, 5, n)\n",
    "train_accuracy = np.zeros(n)\n",
    "val_accuracy = np.zeros(n)\n",
    "\n",
    "for ind, C in enumerate(C_arr):    \n",
    "    \n",
    "    logreg = LogisticRegression(solver='newton-cg', C=10**C)\n",
    "    logreg.fit(x_train, y_train)\n",
    "\n",
    "    \n",
    "    # Evaluate train & test performance\n",
    "    output = logreg.predict(x_train)\n",
    "    target = y_train\n",
    "    train_accuracy[ind] =sum(output == target)/ len(target)\n",
    "\n",
    "    output = logreg.predict(x_val)\n",
    "    target = y_val\n",
    "    val_accuracy[ind] =sum(output == target)/ len(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "plt.figure(figsize=(18, 8))\n",
    "\n",
    "plt.plot(C_arr, train_accuracy, 'bo-', label=r'accuracy training set', color=\"violet\", alpha=0.6, linewidth=3)\n",
    "plt.plot(C_arr, val_accuracy, 'bo-', label=r'accuracy val set', color=\"darkviolet\", alpha=0.6, linewidth=3)\n",
    "\n",
    "plt.xlabel('C value'); plt.ylabel(r'accuracy')\n",
    "plt.title(r'Evaluate logistic regression with different C')\n",
    "plt.legend(loc='best')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.argmax(val_accuracy)\n",
    "C_best= C_arr[index]\n",
    "\n",
    "logreg = LogisticRegression(solver = 'newton-cg',C=10**C_best)\n",
    "logreg.fit(x_train, y_train)\n",
    "coefficients = logreg.coef_\n",
    "\n",
    "print('Logistic Regression')\n",
    "\n",
    "output = logreg.predict(x_train)\n",
    "target = y_train\n",
    "accuracy = sum(output == target)/ len(target)\n",
    "print(\"\\nAccuracy on train set: {:.2f} %\".format(accuracy*100))\n",
    "\n",
    "output = logreg.predict(x_val)\n",
    "target = y_val\n",
    "accuracy = sum(output == target)/ len(target)\n",
    "print(\"\\nAccuracy on validation set: {:.2f} %\".format(accuracy*100))\n",
    "\n",
    "\n",
    "output = logreg.predict(x_test)\n",
    "target = y_test\n",
    "accuracy = sum(output == target)/ len(target)\n",
    "print(\"\\nAccuracy on validation set: {:.2f} %\".format(accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axs= plt.subplots(1,3, figsize=(15,5))\n",
    "axs[0].plot(coefficients[0],c='indigo')\n",
    "axs[1].plot(coefficients[1],c='indigo')\n",
    "axs[2].plot(coefficients[2],c='indigo')\n",
    "\n",
    "axs[0].set_title(\"coefficients for class 0 stars\")\n",
    "axs[1].set_title(\"coefficients for class 1 stars\")\n",
    "axs[2].set_title(\"coefficients for class 2 stars\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Check, what features have highest and lowest coefficients in logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(data.columns)[:-1]\n",
    "\n",
    "def get_most_correlated(coeffs, features, treshold):\n",
    "    coeffs, features = zip(*sorted(zip( coeffs, features)))  \n",
    "\n",
    "    for coef, feat in zip(coeffs, features):\n",
    "        if abs(coef) > treshold:\n",
    "            print(\"coef = {:.4f} \\tfor\\t {:}\".format(coef, feat))\n",
    "            \n",
    "            \n",
    "def get_least_correlated(coeffs, features, treshold):\n",
    "    coeffs, features = zip(*sorted(zip( coeffs, features)))  ## sorting elements in (val, freq) indexing on \n",
    "\n",
    "    for coef, feat in zip(coeffs, features):\n",
    "        if abs(coef) < treshold:\n",
    "            print(\"coef = {:.4f} \\tfor\\t {:}\".format(coef, feat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## most important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Most correlated for 0 star\")\n",
    "get_most_correlated(coefficients[0], features, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Most correlated for 1 star\")\n",
    "get_most_correlated(coefficients[1], features, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Most correlated for 2 star\")\n",
    "get_most_correlated(coefficients[2], features, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### less important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Least correlated for 0 star\")\n",
    "get_least_correlated(coefficients[0], features, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Least correlated for 0 star\")\n",
    "get_least_correlated(coefficients[2], features, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Least correlated for 0 star\")\n",
    "get_least_correlated(coefficients[2], features, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment:\n",
    "\n",
    "Linear regression and regularized linear regression are not suitable for this particular task.\n",
    "From analisys of coefficients in logistic regression the following seems to be not important:\n",
    "* cuisine_x_Bar\n",
    "* cuisine_x_Italian\n",
    "* cuisine_y_Pizzeria\n",
    "\n",
    "But, stutus \"widow\" is highly correlated with \"0\" and \"1\" star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
